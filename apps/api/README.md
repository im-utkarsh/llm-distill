# âš¡ï¸ High-Performance LLM Streaming API

This directory contains the FastAPI backend application responsible for serving the distilled `gemma-2b-it` model. It's designed for high throughput and real-time responsiveness using a non-blocking, queued architecture and Server-Sent Events (SSE) for streaming.

For a high-level overview of the entire project, please refer to the [**root README.md**](../../README.md).

---

## âœ¨ Features

-   **Asynchronous by Design:** Built with FastAPI and Starlette for high performance under concurrent loads.
-   **Real-Time Token Streaming:** Uses Server-Sent Events (SSE) to stream the model's generated tokens to the client as they are produced, creating a responsive user experience.
-   **Queued Inference System:** Implements a FIFO (First-In, First-Out) queue to manage incoming inference requests, preventing the server from being overloaded and ensuring fair processing.
-   **Non-Blocking Model Inference:** Model generation, a blocking I/O-bound operation, is run in a separate thread to avoid freezing the main application's event loop.
-   **Graceful Connection Handling:** Manages client connections and provides a mechanism to cancel in-flight inference jobs if a client disconnects.
-   **Dockerized:** Comes with a multi-stage `Dockerfile` for building a lean, production-ready container image.

---

## ğŸ› ï¸ Local Development Setup

### Prerequisites

-   Python 3.10+
-   A virtual environment tool (`venv`).

### Installation and Running

1.  **Navigate to the API directory:**
    ```bash
    cd apps/api
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Run the development server:**
    ```bash
    uvicorn app.main:app --host 0.0.0.0 --port 7860 --reload
    ```

The API server will start on `http://localhost:7860`. On the first run, the distilled model (`im-utkarsh/distilled-gemma-squad-model`) will be downloaded and cached from the Hugging Face Hub.

---

## ğŸ³ Running with Docker

The API is fully containerized for easy deployment and isolation.

1.  **Build the Docker image:**
    From the `apps/api` directory, run:
    ```bash
    docker build -t distilled-llm-api .
    ```

2.  **Run the Docker container:**
    ```bash
    docker run -p 7860:7860 --gpus all distilled-llm-api
    ```
    *Note: The `--gpus all` flag is required if you have an NVIDIA GPU and want to leverage it for inference inside the container. If you are running on a CPU, you can omit this flag.*

The containerized API will be accessible at `http://localhost:7860`.

---

## ğŸ”Œ API Endpoints

The API follows a two-step process for streaming responses: first submit a job, then connect to a stream to receive the results.

### 1. Submit an Inference Job

-   **Endpoint:** `POST /api/v1/chat/submit`
-   **Description:** Accepts a prompt and context, placing them into the inference queue. A `client_id` must first be established by connecting to the stream endpoint.
-   **Request Body:**
    ```json
    {
      "prompt": "What is knowledge distillation?",
      "context": "Knowledge distillation is a process in machine learning where a compact model, the student, is trained to reproduce the behavior of a larger, pre-trained model, the teacher.",
      "client_id": "some-unique-client-identifier"
    }
    ```

### 2. Stream Chat Responses

-   **Endpoint:** `GET /api/v1/chat/stream/{client_id}`
-   **Description:** Establishes a long-lived Server-Sent Event (SSE) connection. The client connects to this endpoint *first* to register itself and get a dedicated response queue. After connecting, it calls the `/submit` endpoint. Tokens from the model will be streamed back through this connection.
-   **Path Parameter:**
    -   `client_id` (string, required): A unique ID generated by the client (e.g., a UUID) to identify its connection.

---

## ğŸ—ï¸ Architectural Overview

-   **`main.py`**: The FastAPI application entry point. It manages application state (inference queue, connection manager) and the background worker task during its [lifespan](https://fastapi.tiangolo.com/advanced/events/).
-   **`api/routes.py`**: Defines the two public-facing HTTP endpoints (`/chat/submit` and `/chat/stream/{client_id}`).
-   **`llm/connection_manager.py`**: A stateful class that keeps track of active SSE client connections. Each connected client is given a dedicated `asyncio.Queue` to receive its response tokens.
-   **`llm/services.py`**: Contains the core logic.
    -   `fifo_inference_worker`: An async background task that continuously pulls jobs from the main `inference_queue`.
    -   `run_inference_sync`: A synchronous function that runs the blocking `model.generate()` call in a thread pool executor (`loop.run_in_executor`), preventing it from blocking the main asyncio event loop. It streams tokens back to the client's dedicated queue.
-   **`llm/model.py`**: Handles the loading of the distilled model and tokenizer from the Hugging Face Hub.

---

## ğŸ“‚ Directory Structure

````

api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes.py         \# API endpoint definitions
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py         \# Application settings (e.g., CORS)
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ connection\_manager.py \# Manages client SSE connections
â”‚   â”‚   â”œâ”€â”€ model.py            \# Loads the Hugging Face model
â”‚   â”‚   â””â”€â”€ services.py         \# Inference worker and queuing logic
â”‚   â””â”€â”€ main.py                 \# FastAPI app entry point
â”œâ”€â”€ Dockerfile                  \# For building the production container
â””â”€â”€ requirements.txt            \# Python dependencies